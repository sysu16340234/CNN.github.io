# 卷积神经网络学习 --论文《ImageNet Classification with Deep Convolutional Neural Network》阅读


## 1. 卷积神经网络基本结构
### 1.1. 卷积层
卷积层的作用是对输入的图像进行初步的特征提取,即卷积操作,实现方法是使用数个卷积核对输入图像进行卷积操作,得到图像的数组特征,对于卷积的介绍如下:

卷积的过程是使用一个固定大小的nxn矩阵在图像矩阵上以固定步长进行滑窗操作,每一次滑窗操作将卷积核与图像矩阵重叠的元素进行乘积再求和,最后得到的就是特征矩阵,用来滑窗的nxn矩阵称为卷积核,以下图为例,卷积核初始位于输入矩阵的左上角,这时我们计算卷积的结果为3 * 1 + 0* 1 + -1 * 1 + 1 * 1 + 0 * 0 + 7 * -1 + 2 * 1 + 3 * 0 +5 * -1 = -7;

![convolution](https://cdn-images-1.medium.com/max/1600/1*7S266Kq-UCExS25iX_I_AQ.png)

需要注意的是,为了使输出图像的行列满足要求,我们可能会在输入矩阵外部添加一定宽度的padding,从而使卷积结果满足要求


### 1.2. 池化层
池化层的作用是对输入的矩阵进行降采样,从而降低参数的数量,进一步提取特征,池化层的操作方法与卷积类似,使用一个nxn的窗口以一定的步长在矩阵上进行滑动,窗口内的元素以一定的规则取样出一个新的值(常用的方法有极大值池化和均值池化等),最终得到一个降采样后的特征矩阵,下面是一个使用2x2的窗口以2为步长在矩阵上进行极大值采样的结果:

![polling](https://upload.wikimedia.org/wikipedia/commons/thumb/e/e9/Max_pooling.png/471px-Max_pooling.png)

### 1.3. 全连接层
全连接层的每一个输入神经元与上一层的输出神经元进行全连接,连接方法是对上一层的输出进行加权求和并加上一个偏置值b;

### 1.4. 激活函数层
激活函数层一般会跟在全连接层,池化层和全连接层之后,目的是将这些层的输出映射到一定的范围内,以满足要求,以sigmoid函数为例,sigmoid函数会将输入映射到(0,1)区间内:

![sigmoid](https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/Logistic-curve.svg/330px-Logistic-curve.svg.png)

### 1.5. 损失函数层
损失函数层通过计算预测值与真实值之间的偏差来对神经网络进行更新,一般来说,神经网络需要达到的目的就是最小化这个损失函数.

## 2. 论文分析
### 2.1. 论文中设计的卷积神经网络结构
#### 2.1.1. 总体结构
这是作者所设计的卷积神经网络的基本结构:
![alex](https://cdn-images-1.medium.com/max/1600/1*qyc21qM0oxWEuRaj-XJKcw.png)

包含前五个卷积层和后三个全连接层,其中第二层,第三层和第五层后有池化层,池化使用的是重叠最大池化,作者提出采用重叠池化而非传统非重叠池化的目的是降低过拟合程度,提升模型的泛用性;

#### 2.1.2 激活函数
作者采用的激活函数是ReLU函数,或称线性整流函数,作者采用ReLU的理由是ReLU函数比传统的tanh函数或sigmoid函数更加快速;

![relu](https://upload.wikimedia.org/wikipedia/commons/thumb/c/c9/Ramp_function.svg/1920px-Ramp_function.svg.png)

#### 2.1.3 局部响应归一化(LRN)
局部响应归一化来自于神经元的局部抑制现象,作者指出使用LRN作为输入将有效提高识别率,LRN的方法如下:
![LRN](https://img-blog.csdn.net/20180202152857533)

其中b为对a进行归一化的结果,b和a的上下标意义为在位置(x,y)处的第i个核,k,n,α,β是超参数,文章使用的参数值为k = 2, n = 5, α = 10^−4 
,  β = 0.75.

### 2.2 避免过拟合的方法
#### 2.2.1 数据扩充
数据扩充是最简单有效的避免过拟合的方法,文章中采用了两种方法进行数据扩充

第一种方法是采用图像平移或水平镜像的方法在目标图像中随机提取若干个块并对它们的预测值作平均;

第二种方法是增强RGB通道的值,通过PCA将主成分添加到RGB通道上完成;

#### 2.2.2 dropout
dropout的基本思想是以一定的概率将神经元的输出设置为0(文中这个概率为0.5),这样这个神经元的输出就参与之后的传播,这有效降低了神经网络的复杂度,而且由于被放弃的神经元可能是多余的,这样做能够有效防止过拟合;

#### 2.2.3 学习方法
文中采用的是随机梯度下降法,提到的学习规则如下:

![learn](https://github.com/sysu16340234/cnn.github.io/blob/master/20190315201846.png?raw=true)

采用了动量和衰减来进行学习,作者特别指出,0.0005的衰减能够有效降低错误率.公式中v为动量变量,第i + 1步的v由动量0.9乘以第i步的v减去学习率乘以衰减常数和第i步的权重再减去学习率乘以第i批次的损失函数对权重的导数的平均数
